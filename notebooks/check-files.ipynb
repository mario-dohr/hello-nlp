{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll try to build a small model that can predict the next word given a sequence of words.\n",
    "I use pretrained word embedings from [here](https://nlp.stanford.edu/projects/glove/). To train the model I use Project Gutenberg.\n",
    "Todos:\n",
    "* Load the training data and clean it\n",
    "* create embedding matrix from word2vec embeddings\n",
    "* create word_to_idx and idx_to_word dict, include <pad> and <unk>\n",
    "* create idx_from_word func (should return the index of <unk> for unknown words\n",
    "* create training sequences:\n",
    "  * sequences should have the length of the largest sentence. If sentence is shorter, then it is padded with <pad>\n",
    "  * each word should predict the next and the last predict <pad>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get text and create the word list (list of tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\dohr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "['ca01',\n 'ca02',\n 'ca03',\n 'ca04',\n 'ca05',\n 'ca06',\n 'ca07',\n 'ca08',\n 'ca09',\n 'ca10',\n 'ca11',\n 'ca12',\n 'ca13',\n 'ca14',\n 'ca15',\n 'ca16',\n 'ca17',\n 'ca18',\n 'ca19',\n 'ca20',\n 'ca21',\n 'ca22',\n 'ca23',\n 'ca24',\n 'ca25',\n 'ca26',\n 'ca27',\n 'ca28',\n 'ca29',\n 'ca30',\n 'ca31',\n 'ca32',\n 'ca33',\n 'ca34',\n 'ca35',\n 'ca36',\n 'ca37',\n 'ca38',\n 'ca39',\n 'ca40',\n 'ca41',\n 'ca42',\n 'ca43',\n 'ca44',\n 'cb01',\n 'cb02',\n 'cb03',\n 'cb04',\n 'cb05',\n 'cb06',\n 'cb07',\n 'cb08',\n 'cb09',\n 'cb10',\n 'cb11',\n 'cb12',\n 'cb13',\n 'cb14',\n 'cb15',\n 'cb16',\n 'cb17',\n 'cb18',\n 'cb19',\n 'cb20',\n 'cb21',\n 'cb22',\n 'cb23',\n 'cb24',\n 'cb25',\n 'cb26',\n 'cb27',\n 'cc01',\n 'cc02',\n 'cc03',\n 'cc04',\n 'cc05',\n 'cc06',\n 'cc07',\n 'cc08',\n 'cc09',\n 'cc10',\n 'cc11',\n 'cc12',\n 'cc13',\n 'cc14',\n 'cc15',\n 'cc16',\n 'cc17',\n 'cd01',\n 'cd02',\n 'cd03',\n 'cd04',\n 'cd05',\n 'cd06',\n 'cd07',\n 'cd08',\n 'cd09',\n 'cd10',\n 'cd11',\n 'cd12',\n 'cd13',\n 'cd14',\n 'cd15',\n 'cd16',\n 'cd17',\n 'ce01',\n 'ce02',\n 'ce03',\n 'ce04',\n 'ce05',\n 'ce06',\n 'ce07',\n 'ce08',\n 'ce09',\n 'ce10',\n 'ce11',\n 'ce12',\n 'ce13',\n 'ce14',\n 'ce15',\n 'ce16',\n 'ce17',\n 'ce18',\n 'ce19',\n 'ce20',\n 'ce21',\n 'ce22',\n 'ce23',\n 'ce24',\n 'ce25',\n 'ce26',\n 'ce27',\n 'ce28',\n 'ce29',\n 'ce30',\n 'ce31',\n 'ce32',\n 'ce33',\n 'ce34',\n 'ce35',\n 'ce36',\n 'cf01',\n 'cf02',\n 'cf03',\n 'cf04',\n 'cf05',\n 'cf06',\n 'cf07',\n 'cf08',\n 'cf09',\n 'cf10',\n 'cf11',\n 'cf12',\n 'cf13',\n 'cf14',\n 'cf15',\n 'cf16',\n 'cf17',\n 'cf18',\n 'cf19',\n 'cf20',\n 'cf21',\n 'cf22',\n 'cf23',\n 'cf24',\n 'cf25',\n 'cf26',\n 'cf27',\n 'cf28',\n 'cf29',\n 'cf30',\n 'cf31',\n 'cf32',\n 'cf33',\n 'cf34',\n 'cf35',\n 'cf36',\n 'cf37',\n 'cf38',\n 'cf39',\n 'cf40',\n 'cf41',\n 'cf42',\n 'cf43',\n 'cf44',\n 'cf45',\n 'cf46',\n 'cf47',\n 'cf48',\n 'cg01',\n 'cg02',\n 'cg03',\n 'cg04',\n 'cg05',\n 'cg06',\n 'cg07',\n 'cg08',\n 'cg09',\n 'cg10',\n 'cg11',\n 'cg12',\n 'cg13',\n 'cg14',\n 'cg15',\n 'cg16',\n 'cg17',\n 'cg18',\n 'cg19',\n 'cg20',\n 'cg21',\n 'cg22',\n 'cg23',\n 'cg24',\n 'cg25',\n 'cg26',\n 'cg27',\n 'cg28',\n 'cg29',\n 'cg30',\n 'cg31',\n 'cg32',\n 'cg33',\n 'cg34',\n 'cg35',\n 'cg36',\n 'cg37',\n 'cg38',\n 'cg39',\n 'cg40',\n 'cg41',\n 'cg42',\n 'cg43',\n 'cg44',\n 'cg45',\n 'cg46',\n 'cg47',\n 'cg48',\n 'cg49',\n 'cg50',\n 'cg51',\n 'cg52',\n 'cg53',\n 'cg54',\n 'cg55',\n 'cg56',\n 'cg57',\n 'cg58',\n 'cg59',\n 'cg60',\n 'cg61',\n 'cg62',\n 'cg63',\n 'cg64',\n 'cg65',\n 'cg66',\n 'cg67',\n 'cg68',\n 'cg69',\n 'cg70',\n 'cg71',\n 'cg72',\n 'cg73',\n 'cg74',\n 'cg75',\n 'ch01',\n 'ch02',\n 'ch03',\n 'ch04',\n 'ch05',\n 'ch06',\n 'ch07',\n 'ch08',\n 'ch09',\n 'ch10',\n 'ch11',\n 'ch12',\n 'ch13',\n 'ch14',\n 'ch15',\n 'ch16',\n 'ch17',\n 'ch18',\n 'ch19',\n 'ch20',\n 'ch21',\n 'ch22',\n 'ch23',\n 'ch24',\n 'ch25',\n 'ch26',\n 'ch27',\n 'ch28',\n 'ch29',\n 'ch30',\n 'cj01',\n 'cj02',\n 'cj03',\n 'cj04',\n 'cj05',\n 'cj06',\n 'cj07',\n 'cj08',\n 'cj09',\n 'cj10',\n 'cj11',\n 'cj12',\n 'cj13',\n 'cj14',\n 'cj15',\n 'cj16',\n 'cj17',\n 'cj18',\n 'cj19',\n 'cj20',\n 'cj21',\n 'cj22',\n 'cj23',\n 'cj24',\n 'cj25',\n 'cj26',\n 'cj27',\n 'cj28',\n 'cj29',\n 'cj30',\n 'cj31',\n 'cj32',\n 'cj33',\n 'cj34',\n 'cj35',\n 'cj36',\n 'cj37',\n 'cj38',\n 'cj39',\n 'cj40',\n 'cj41',\n 'cj42',\n 'cj43',\n 'cj44',\n 'cj45',\n 'cj46',\n 'cj47',\n 'cj48',\n 'cj49',\n 'cj50',\n 'cj51',\n 'cj52',\n 'cj53',\n 'cj54',\n 'cj55',\n 'cj56',\n 'cj57',\n 'cj58',\n 'cj59',\n 'cj60',\n 'cj61',\n 'cj62',\n 'cj63',\n 'cj64',\n 'cj65',\n 'cj66',\n 'cj67',\n 'cj68',\n 'cj69',\n 'cj70',\n 'cj71',\n 'cj72',\n 'cj73',\n 'cj74',\n 'cj75',\n 'cj76',\n 'cj77',\n 'cj78',\n 'cj79',\n 'cj80',\n 'ck01',\n 'ck02',\n 'ck03',\n 'ck04',\n 'ck05',\n 'ck06',\n 'ck07',\n 'ck08',\n 'ck09',\n 'ck10',\n 'ck11',\n 'ck12',\n 'ck13',\n 'ck14',\n 'ck15',\n 'ck16',\n 'ck17',\n 'ck18',\n 'ck19',\n 'ck20',\n 'ck21',\n 'ck22',\n 'ck23',\n 'ck24',\n 'ck25',\n 'ck26',\n 'ck27',\n 'ck28',\n 'ck29',\n 'cl01',\n 'cl02',\n 'cl03',\n 'cl04',\n 'cl05',\n 'cl06',\n 'cl07',\n 'cl08',\n 'cl09',\n 'cl10',\n 'cl11',\n 'cl12',\n 'cl13',\n 'cl14',\n 'cl15',\n 'cl16',\n 'cl17',\n 'cl18',\n 'cl19',\n 'cl20',\n 'cl21',\n 'cl22',\n 'cl23',\n 'cl24',\n 'cm01',\n 'cm02',\n 'cm03',\n 'cm04',\n 'cm05',\n 'cm06',\n 'cn01',\n 'cn02',\n 'cn03',\n 'cn04',\n 'cn05',\n 'cn06',\n 'cn07',\n 'cn08',\n 'cn09',\n 'cn10',\n 'cn11',\n 'cn12',\n 'cn13',\n 'cn14',\n 'cn15',\n 'cn16',\n 'cn17',\n 'cn18',\n 'cn19',\n 'cn20',\n 'cn21',\n 'cn22',\n 'cn23',\n 'cn24',\n 'cn25',\n 'cn26',\n 'cn27',\n 'cn28',\n 'cn29',\n 'cp01',\n 'cp02',\n 'cp03',\n 'cp04',\n 'cp05',\n 'cp06',\n 'cp07',\n 'cp08',\n 'cp09',\n 'cp10',\n 'cp11',\n 'cp12',\n 'cp13',\n 'cp14',\n 'cp15',\n 'cp16',\n 'cp17',\n 'cp18',\n 'cp19',\n 'cp20',\n 'cp21',\n 'cp22',\n 'cp23',\n 'cp24',\n 'cp25',\n 'cp26',\n 'cp27',\n 'cp28',\n 'cp29',\n 'cr01',\n 'cr02',\n 'cr03',\n 'cr04',\n 'cr05',\n 'cr06',\n 'cr07',\n 'cr08',\n 'cr09']"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.set_proxy('http://proxy.brz.gv.at:8080')\n",
    "nltk.download('brown')\n",
    "nltk.corpus.brown.fileids()\n",
    "#nltk.corpus.gutenberg.fileids()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dohr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "book_name = 'cp07'\n",
    "nltk.download('punkt')\n",
    "sentences = [[w for w in (map(lambda w: w.lower().strip('_-.,?.!#'), s)) if w.isalnum()] for s in  nltk.corpus.brown.sents(book_name)]\n",
    "corpus_set = set([w for s in sentences for w in s])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the embbeddings\n",
    "\n",
    "Load the embbeddings in a dictionary **word_vecs_en**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad\n",
      "unk\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([703, 50])"
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vecs_en = {}\n",
    "v_list = []\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "with io.open('../glove.6B.50d.txt','r', encoding='utf8') as f:\n",
    "    idx = 0\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        w = tokens[0]\n",
    "        if w in corpus_set or w == 'unk' or w == 'pad':\n",
    "            if w == 'unk' or w == 'pad':\n",
    "                print(w)\n",
    "            v = torch.FloatTensor(list(map(float,tokens[1:])))\n",
    "            v_list.append(v)\n",
    "            word_to_idx[w] = idx\n",
    "            idx_to_word[idx] = w\n",
    "            word_vecs_en[w] = v\n",
    "            idx += 1\n",
    "\n",
    "\n",
    "embeddings_matrix = torch.stack(v_list)\n",
    "embeddings_matrix.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get text and create the word list (list of tokens)\n",
    "\n",
    "Get the available texts from Projekt Gutenberg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [
    {
     "data": {
      "text/plain": "(700, 3)"
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idx_from_word(word):\n",
    "    if word in word_to_idx:\n",
    "        return word_to_idx[word]\n",
    "    else:\n",
    "        return word_to_idx['unk']\n",
    "\n",
    "idx_from_word('hudri'), idx_from_word('and')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all words in the text: 706\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([3, 3, 3, 3, 3, 4])"
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Number of all words in the text: {len(corpus_set)}')\n",
    "max([len(s) for s in sentences])\n",
    "l = [3]*5\n",
    "l.extend([4])\n",
    "torch.LongTensor(l)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the dataset to load the input sequences from the sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[566, 566, 566, 566, 566, 566, 566, 566,  99, 194,   4,   0, 328, 292,\n           26, 396, 362,   2, 540, 641,  88, 690,   1,  58, 196,   1, 395,  70,\n           14,   2,  21, 155, 534, 397,  20, 321,   2, 547,  12,  73],\n         [566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566,\n          566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566,\n          566, 566, 566, 485, 673,  26, 610,  17,  97, 383,   3, 693]]),\n tensor([[566, 566, 566, 566, 566, 566, 566, 566, 194,   4,   0, 328, 292,  26,\n          396, 362,   2, 540, 641,  88, 690,   1,  58, 196,   1, 395,  70,  14,\n            2,  21, 155, 534, 397,  20, 321,   2, 547,  12,  73, 566],\n         [566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566,\n          566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566,\n          566, 566, 566, 673,  26, 610,  17,  97, 383,   3, 693, 566]]))"
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,sents):\n",
    "        \"\"\"\n",
    "        :param sents: the list of sentences\n",
    "        \"\"\"\n",
    "        L = max([len(s) for s in sents])\n",
    "        self.input = torch.LongTensor(len(sents),L)\n",
    "        self.target = torch.LongTensor(len(sents),L)\n",
    "        for i, s in enumerate(sents):\n",
    "            inp = [idx_from_word('pad')]*(L-len(s))\n",
    "            tar = [idx_from_word('pad')]*(L-len(s))\n",
    "            w_list = []\n",
    "            for w in s:\n",
    "                w_list.append(idx_from_word(w))\n",
    "            inp.extend(w_list)\n",
    "            tar.extend(w_list[1:])\n",
    "            tar.append(idx_from_word('pad'))\n",
    "            self.input[i] = torch.LongTensor(inp)\n",
    "            self.target[i] = torch.LongTensor(tar)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.target[idx]\n",
    "\n",
    "dataset = Dataset(sentences)\n",
    "dataset[10:12]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "create the function `word_to_vec` which tuns a word or a list of words into vectors. If a word is not in the embbeddings the function returns a zero vector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Build the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,out_size,embed_mat,num_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embed_mat)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = True\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size,out_size)\n",
    "\n",
    "    def init_hidden(self,batch_size=1):\n",
    "        return  torch.zeros((self.num_layers,batch_size,self.hidden_size)),torch.zeros((self.num_layers,batch_size,self.hidden_size))\n",
    "\n",
    "    def forward(self,input,hidden):\n",
    "        x = self.embeddings(input)\n",
    "        out, hidden = self.rnn(x,hidden)\n",
    "        out = self.linear1(out)\n",
    "        dropout = nn.Dropout(p=0.2)\n",
    "        out = dropout(out)\n",
    "        out = torch.relu(out)\n",
    "        return  self.linear2(out),hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create the dataset for loading the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-Step: 0-0, elapsed time: 4.570958  loss: 6.558311\n",
      "Epoch-Step: 0-5, elapsed time: 22.861003  loss: 6.299108\n",
      "Epoch-Step: 0-10, elapsed time: 42.276511  loss: 5.265857\n",
      "Epoch-Step: 0-15, elapsed time: 61.428453  loss: 5.383120\n",
      "Epoch-Step: 1-0, elapsed time: 72.550402  loss: 4.744092\n",
      "Epoch-Step: 1-5, elapsed time: 90.900595  loss: 5.823966\n",
      "Epoch-Step: 1-10, elapsed time: 110.544825  loss: 5.059399\n",
      "Epoch-Step: 1-15, elapsed time: 129.369892  loss: 5.058974\n",
      "Epoch-Step: 2-0, elapsed time: 140.655609  loss: 4.574659\n",
      "Epoch-Step: 2-5, elapsed time: 159.486790  loss: 5.646011\n",
      "Epoch-Step: 2-10, elapsed time: 177.797986  loss: 5.019992\n",
      "Epoch-Step: 2-15, elapsed time: 196.617591  loss: 4.947995\n",
      "Epoch-Step: 3-0, elapsed time: 208.097561  loss: 4.550393\n",
      "Epoch-Step: 3-5, elapsed time: 226.196033  loss: 5.682522\n",
      "Epoch-Step: 3-10, elapsed time: 244.834483  loss: 4.967468\n",
      "Epoch-Step: 3-15, elapsed time: 263.874659  loss: 5.044714\n",
      "Epoch-Step: 4-0, elapsed time: 278.278709  loss: 4.592836\n",
      "Epoch-Step: 4-5, elapsed time: 296.765410  loss: 5.669716\n",
      "Epoch-Step: 4-10, elapsed time: 315.838514  loss: 4.953334\n",
      "Epoch-Step: 4-15, elapsed time: 336.607849  loss: 5.041366\n",
      "Epoch-Step: 5-0, elapsed time: 347.618614  loss: 4.650528\n",
      "Epoch-Step: 5-5, elapsed time: 366.243511  loss: 5.665427\n",
      "Epoch-Step: 5-10, elapsed time: 384.642034  loss: 4.920583\n",
      "Epoch-Step: 5-15, elapsed time: 403.043314  loss: 5.071129\n",
      "Epoch-Step: 6-0, elapsed time: 413.989131  loss: 4.584976\n",
      "Epoch-Step: 6-5, elapsed time: 430.796233  loss: 5.599198\n",
      "Epoch-Step: 6-10, elapsed time: 447.917638  loss: 5.109499\n",
      "Epoch-Step: 6-15, elapsed time: 464.545329  loss: 4.960382\n",
      "Epoch-Step: 7-0, elapsed time: 474.482642  loss: 4.557143\n",
      "Epoch-Step: 7-5, elapsed time: 490.854063  loss: 5.691012\n",
      "Epoch-Step: 7-10, elapsed time: 507.557468  loss: 4.917979\n",
      "Epoch-Step: 7-15, elapsed time: 524.315246  loss: 5.056024\n",
      "Epoch-Step: 8-0, elapsed time: 534.618458  loss: 4.569122\n",
      "Epoch-Step: 8-5, elapsed time: 553.012533  loss: 5.645574\n",
      "Epoch-Step: 8-10, elapsed time: 570.214716  loss: 4.984141\n",
      "Epoch-Step: 8-15, elapsed time: 587.939977  loss: 4.991755\n",
      "Epoch-Step: 9-0, elapsed time: 598.664622  loss: 4.580167\n",
      "Epoch-Step: 9-5, elapsed time: 616.720883  loss: 5.611073\n",
      "Epoch-Step: 9-10, elapsed time: 633.621873  loss: 5.000703\n",
      "Epoch-Step: 9-15, elapsed time: 650.376641  loss: 4.953773\n",
      "Finished in 657.329880 seconds\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "lr = 5e-3\n",
    "batch_size = 8\n",
    "\n",
    "#init dataset and data loader\n",
    "dataset = Dataset(sentences)\n",
    "data_loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size)\n",
    "\n",
    "# init the model\n",
    "model = Model(input_size=50,hidden_size=1024,num_layers=5,out_size=len(word_to_idx),embed_mat=embeddings_matrix)\n",
    "\n",
    "# init the optimizer\n",
    "optim = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "# init the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "# the training loop\n",
    "start = time.time()\n",
    "for i in range(n_epochs):\n",
    "    step = 0\n",
    "    h = model.init_hidden(batch_size=batch_size)\n",
    "    for train_x, train_y in data_loader:\n",
    "        if len(train_x) < batch_size:\n",
    "            continue\n",
    "        h = tuple([e.data  for e in h])\n",
    "        #print('shape x:',train_x.shape)\n",
    "        #print('shape y:',train_y.shape)\n",
    "        #print('type y:',train_y.type())\n",
    "        optim.zero_grad()\n",
    "        pred,h = model(train_x,h)\n",
    "        #print('pred:',pred.shape)\n",
    "        loss = criterion(pred.reshape(batch_size,len(word_to_idx),40),train_y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if step%5 == 0:\n",
    "            print('Epoch-Step: %d-%d, elapsed time: %f  loss: %f'%(i,step,(time.time()-start),loss))\n",
    "        step += 1\n",
    "\n",
    "print('Finished in %f seconds'%(time.time()-start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('Hallo')\n",
    "    model.eval()\n",
    "    model.init_hidden(1)\n",
    "    pred = model(dataset[5][0].reshape(1,-1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([])"
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "pred.new()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "pred_idx = pred.topk(1,dim=2)[1].squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[149, 154, 156, 157, 154, 157, 144, 153, 144, 145, 154, 149, 157, 154,\n         157, 153, 149, 153, 144, 153, 157, 144, 149, 157, 145, 144, 156, 153,\n         144, 157, 144, 144, 157, 154, 154, 153, 156, 153, 146, 157],\n        [157, 157, 144, 144, 145, 157, 157, 154, 144, 154, 152, 152, 144, 154,\n         154, 154, 153, 153, 157, 151, 144, 154, 147, 147, 152, 152, 153, 147,\n         147, 144, 144, 149, 144, 147, 157, 157, 157, 156, 149, 152],\n        [144, 157, 153, 149, 157, 145, 144, 147, 153, 154, 147, 157, 144, 154,\n         157, 157, 157, 153, 158, 157, 157, 144, 157, 147, 144, 157, 157, 147,\n         146, 153, 157, 147, 145, 153, 144, 144, 144, 153, 157, 144],\n        [157, 157, 144, 144, 156, 157, 152, 157, 157, 144, 156, 144, 144, 158,\n         156, 149, 153, 144, 145, 144, 144, 144, 147, 144, 147, 147, 145, 157,\n         144, 158, 154, 157, 156, 155, 147, 156, 147, 152, 154, 156],\n        [147, 157, 157, 157, 153, 146, 144, 147, 144, 144, 144, 153, 154, 151,\n         158, 144, 152, 151, 154, 144, 147, 154, 157, 157, 154, 157, 158, 147,\n         158, 153, 156, 157, 147, 157, 144, 153, 157, 145, 149, 153],\n        [144, 152, 157, 144, 156, 144, 154, 152, 147, 156, 154, 157, 144, 144,\n         144, 157, 153, 157, 144, 156, 157, 152, 154, 149, 154, 157, 149, 144,\n         147, 154, 157, 154, 153, 157, 144, 154, 144, 151, 149, 144],\n        [157, 157, 154, 152, 157, 147, 156, 153, 149, 157, 154, 157, 144, 154,\n         154, 157, 154, 149, 150, 158, 144, 144, 147, 157, 144, 157, 154, 157,\n         153, 153, 157, 147, 149, 147, 157, 147, 152, 149, 144, 153],\n        [157, 147, 144, 156, 153, 154, 144, 147, 144, 156, 157, 157, 144, 153,\n         154, 157, 154, 153, 152, 157, 157, 157, 157, 154, 157, 154, 144, 153,\n         149, 157, 157, 147, 183, 144, 147, 157, 157, 154, 153, 144]])"
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [375], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m pred_idx:\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(idx_to_word[\u001B[43mi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m])\n",
      "\u001B[1;31mValueError\u001B[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "for i in pred_idx:\n",
    "    print(idx_to_word[i.item()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [
    {
     "data": {
      "text/plain": "['very', 'well']"
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566,\n         566, 566, 566, 566, 566, 566,  51,  10,   5, 523,  17,   0, 349,   3,\n         507, 331,  12,   0, 302,   7, 697, 561,   2, 129,   0, 308]),\n tensor([566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566,\n         566, 566, 566, 566, 566, 566,  10,   5, 523,  17,   0, 349,   3, 507,\n         331,  12,   0, 302,   7, 697, 561,   2, 129,   0, 308, 566]))"
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}